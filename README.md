# honkhonk

honk honk, honk honk honk honk!

# What's up
We use openelm to perform red-teaming prompt evolution. This is a minimal demo of QDAIF in this theme.

Ok, so everything above those `CONFIGSTORE.store(...)` was just to set up the configs. They are self-explanatory and all we may need to change are:
in CustomModelConfig,
 - model_path
in CustomMAPElitesConfig
 - map_grid_size (the number of bins. We are doing 1-dim behavior space determined by the prompt length)
 - init_steps (how many steps to do random generation)
 - total_steps (total_steps - init_steps is the number of steps to mutate)
in RedTeamingConfig
 - batch_size (the number of prompts generated per mutation)


In CustomPromptEvolution, a few methods are overwritten to implement what we are doing. A few things:
 - The `self.fitness_model` is the model used to get the fitness score. So far it rates the sentiment by calling some model and then pass through `get_sentiment_score` function.
 - A genotype consists of
   - "instruction_str" (the current prompt)
   - "old_question" (the prompt of its parent)
   - "old_answer" (an answer generated by LM)
   When mutating, it uses `mutation_instructions` and insert its `instruction_str` in it. A new instruction will be generated and we stick the old instruction and an answer into the new Genotype. This is what `mutate_prompt` does.
 - `random_prompt` is the initial random generation. So far it starts with a fixed element
   ```
   {
     "instruction_str": self.task.instruction_str,
     "old_question": self.task.instruction_str,
     "old_answer": self.task.target,
   }
   ```
 - `evaluate_string` is the function that generates LM response to the prompt. The prompt+response will be fed into `fitness` for sentiment evaluation.